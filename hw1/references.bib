@article {Lake1332,
	author = {Lake, Brenden M. and Salakhutdinov, Ruslan and Tenenbaum, Joshua B.},
	title = {Human-level concept learning through probabilistic program induction},
	volume = {350},
	number = {6266},
	pages = {1332--1338},
	year = {2015},
	doi = {10.1126/science.aab3050},
	publisher = {American Association for the Advancement of Science},
	abstract = {Not only do children learn effortlessly, they do so quickly and with a remarkable ability to use what they have learned as the raw material for creating new stuff. Lake et al. describe a computational model that learns in a similar fashion and does so better than current deep learning algorithms. The model classifies, parses, and recreates handwritten characters, and can generate new letters of the alphabet that look {\textquotedblleft}right{\textquotedblright} as judged by Turing-like tests of the model{\textquoteright}s output in comparison to what real humans produce.Science, this issue p. 1332People learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy. People can also use learned concepts in richer ways than conventional algorithms{\textemdash}for action, imagination, and explanation. We present a computational model that captures these human learning abilities for a large class of simple visual concepts: handwritten characters from the world{\textquoteright}s alphabets. The model represents concepts as simple programs that best explain observed examples under a Bayesian criterion. On a challenging one-shot classification task, the model achieves human-level performance while outperforming recent deep learning approaches. We also present several {\textquotedblleft}visual Turing tests{\textquotedblright} probing the model{\textquoteright}s creative generalization abilities, which in many cases are indistinguishable from human behavior.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/350/6266/1332},
	eprint = {https://science.sciencemag.org/content/350/6266/1332.full.pdf},
	journal = {Science}
}



@InProceedings{pmlr-v48-santoro16,
  title = 	 {Meta-Learning with Memory-Augmented Neural Networks},
  author = 	 {Adam Santoro and Sergey Bartunov and Matthew Botvinick and Daan Wierstra and Timothy Lillicrap},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {1842--1850},
  year = 	 {2016},
  editor = 	 {Maria Florina Balcan and Kilian Q. Weinberger},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/santoro16.pdf},
  url = 	 {http://proceedings.mlr.press/v48/santoro16.html},
  abstract = 	 {Despite recent breakthroughs in the applications of deep neural networks, one setting that presents a persistent challenge is that of "one-shot learning." Traditional gradient-based networks require a lot of data to learn, often through extensive iterative training. When new data is encountered, the models must inefficiently relearn their parameters to adequately incorporate the new information without catastrophic interference. Architectures with augmented memory capacities, such as Neural Turing Machines (NTMs), offer the ability to quickly encode and retrieve new information, and hence can potentially obviate the downsides of conventional models. Here, we demonstrate the ability of a memory-augmented neural network to rapidly assimilate new data, and leverage this data to make accurate predictions after only a few samples. We also introduce a new method for accessing an external memory that focuses on memory content, unlike previous methods that additionally use memory location-based focusing mechanisms.}
}


@article{DBLP:journals/corr/MishraRCA17,
  author    = {Nikhil Mishra and
               Mostafa Rohaninejad and
               Xi Chen and
               Pieter Abbeel},
  title     = {Meta-Learning with Temporal Convolutions},
  journal   = {CoRR},
  volume    = {abs/1707.03141},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.03141},
  archivePrefix = {arXiv},
  eprint    = {1707.03141},
  timestamp = {Mon, 03 Sep 2018 12:15:29 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/MishraRCA17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{Csordas_2019_DNC,
  author    = {Robert Csordas and Jurgen Schmidhuber},
  title     = {Improving Differentiable Neural Computers Through Memory Masking, De-allocation, and Link Distribution Sharpness Control },
  journal   = {ICLR},
  volume    = {abs/1707.03141},
  year      = {2019},
  url       = {https://openreview.net/pdf?id=HyGEM3C9KQ},
}