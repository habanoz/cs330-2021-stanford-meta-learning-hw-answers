@inproceedings{maml,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1126--1135},
  year={2017},
  organization={JMLR. org}
}

@inproceedings{prototypical,
  title={Prototypical networks for few-shot learning},
  author={Snell, Jake and Swersky, Kevin and Zemel, Richard},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4077--4087},
  year={2017}
}

@article{mamlplusplus,
  title={How to train your MAML},
  author={Antoniou, Antreas and Edwards, Harrison and Storkey, Amos},
  journal={arXiv preprint arXiv:1810.09502},
  year={2018}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article {Lake1332,
	author = {Lake, Brenden M. and Salakhutdinov, Ruslan and Tenenbaum, Joshua B.},
	title = {Human-level concept learning through probabilistic program induction},
	volume = {350},
	number = {6266},
	pages = {1332--1338},
	year = {2015},
	doi = {10.1126/science.aab3050},
	publisher = {American Association for the Advancement of Science},
	abstract = {Not only do children learn effortlessly, they do so quickly and with a remarkable ability to use what they have learned as the raw material for creating new stuff. Lake et al. describe a computational model that learns in a similar fashion and does so better than current deep learning algorithms. The model classifies, parses, and recreates handwritten characters, and can generate new letters of the alphabet that look {\textquotedblleft}right{\textquotedblright} as judged by Turing-like tests of the model{\textquoteright}s output in comparison to what real humans produce.Science, this issue p. 1332People learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy. People can also use learned concepts in richer ways than conventional algorithms{\textemdash}for action, imagination, and explanation. We present a computational model that captures these human learning abilities for a large class of simple visual concepts: handwritten characters from the world{\textquoteright}s alphabets. The model represents concepts as simple programs that best explain observed examples under a Bayesian criterion. On a challenging one-shot classification task, the model achieves human-level performance while outperforming recent deep learning approaches. We also present several {\textquotedblleft}visual Turing tests{\textquotedblright} probing the model{\textquoteright}s creative generalization abilities, which in many cases are indistinguishable from human behavior.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/350/6266/1332},
	eprint = {https://science.sciencemag.org/content/350/6266/1332.full.pdf},
	journal = {Science}
}

